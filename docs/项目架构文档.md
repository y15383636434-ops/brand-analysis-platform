# 品牌分析系统 - 项目架构文档

> 最后更新：2026-01-06

## 📋 目录

1. [项目概述](#项目概述)
2. [技术栈](#技术栈)
3. [项目结构](#项目结构)
4. [核心模块](#核心模块)
5. [API架构](#api架构)
6. [数据库设计](#数据库设计)
7. [数据流](#数据流)
8. [部署架构](#部署架构)

---

## 项目概述

**品牌分析系统**是一个全自动化的品牌数据采集、AI分析和报告生成平台。

### 核心功能

- 🔍 **多平台数据采集**：支持小红书、抖音、微博、知乎、B站、快手等平台
- 🤖 **AI智能分析**：使用大语言模型进行情感分析、主题提取、深度洞察
- 📊 **数据可视化**：提供Web界面查看数据和分析结果
- 📄 **报告生成**：自动生成专业的品牌分析报告（PDF/Word/Excel）

### 系统架构图

```
┌─────────────────────────────────────────────────────────────┐
│                     前端 Web 界面                            │
│  - Dashboard统一控制台                                      │
│  - 品牌管理界面                                             │
│  - 数据采集界面                                             │
│  - 数据分析界面                                             │
│  - 报告查看界面                                             │
└────────────────────┬────────────────────────────────────────┘
                     │ HTTP/WebSocket
┌────────────────────▼────────────────────────────────────────┐
│                  FastAPI 应用层                              │
│  - RESTful API                                               │
│  - WebSocket 实时通信                                        │
│  - 认证授权 (JWT)                                            │
└────────────────────┬────────────────────────────────────────┘
                     │
        ┌────────────┼────────────┐
        │            │            │
┌───────▼───┐ ┌──────▼──────┐ ┌──▼──────────┐
│ 数据采集模块 │ │  AI分析模块  │ │ 报告生成模块 │
│            │ │             │ │            │
│ MediaCrawler│ │ LLM API     │ │ 模板引擎   │
│ 多平台爬虫  │ │ NLP分析     │ │ PDF生成    │
└───────┬───┘ └──────┬──────┘ └──┬──────────┘
        │            │            │
        └────────────┼────────────┘
                     │
        ┌────────────▼────────────┐
        │     数据存储层           │
        │  - MySQL (结构化数据)    │
        │  - MongoDB (非结构化)    │
        │  - Redis (缓存/队列)     │
        └─────────────────────────┘
```

---

## 技术栈

### 后端技术

| 技术 | 版本 | 用途 |
|------|------|------|
| Python | 3.11+ | 主要开发语言 |
| FastAPI | latest | Web框架和API服务 |
| SQLAlchemy | latest | ORM框架 |
| Celery | latest | 异步任务队列 |
| MediaCrawler | latest | 多平台数据爬虫 |
| Pydantic | latest | 数据验证 |
| Loguru | latest | 日志管理 |

### 前端技术

| 技术 | 用途 |
|------|------|
| HTML5/CSS3 | 页面结构 |
| JavaScript (ES6+) | 前端逻辑 |
| Fetch API | HTTP请求 |
| Chart.js/ECharts | 数据可视化 |

### 数据库

| 数据库 | 用途 |
|--------|------|
| MySQL | 存储品牌、任务、报告等结构化数据 |
| MongoDB | 存储爬取的原始数据和分析结果 |
| Redis | 缓存和Celery任务队列 |

### AI服务

| 服务 | 用途 | 说明 |
|------|------|------|
| **LLM 聚合网关** | 统一模型接入 | 基于 OneAPI/NewAPI 部署的私有网关，支持动态切换多模型 |
| **接入端点** | API Base URL | `https://xy.xiaoxu030.xyz:8888/v1`（示例，需配置） |
| **支持模型** | 智能分析 | 支持动态切换 GPT-4o, Claude 3.5, Gemini 等 |
| OpenAI API | GPT模型分析 | 直接调用（可选，优先使用聚合网关） |
| Anthropic API | Claude模型分析 | 直接调用（可选，优先使用聚合网关） |
| Google Gemini API | Gemini模型分析 | 直接调用（可选，优先使用聚合网关） |
| 本地LLM | 本地部署模型 | 可选，本地部署的LLM服务 |

---

## 项目结构

```
githup/
├── app/                          # 主应用目录
│   ├── __init__.py
│   ├── main.py                   # FastAPI应用入口
│   │
│   ├── api/                      # API路由
│   │   └── v1/                   # API v1版本
│   │       ├── brands.py         # 品牌管理API
│   │       ├── crawl_tasks.py    # 爬虫任务API
│   │       ├── analysis_tasks.py # 分析任务API
│   │       ├── reports.py       # 报告API
│   │       ├── data_viewer.py    # 数据查看API
│   │       ├── crawler_ui.py     # 爬虫UI API
│   │       ├── mediacrawler_ui.py # MediaCrawler UI API
│   │       ├── data_analysis.py  # 数据分析API
│   │       ├── data_display.py   # 数据展示API
│   │       └── dashboard.py     # 统一控制台API
│   │
│   ├── core/                     # 核心模块
│   │   └── database.py           # 数据库连接和初始化
│   │
│   ├── models/                   # 数据模型
│   │   ├── brand.py              # 品牌模型
│   │   ├── crawl_task.py         # 爬虫任务模型
│   │   ├── analysis_task.py      # 分析任务模型
│   │   └── report.py              # 报告模型
│   │
│   ├── services/                 # 业务服务层
│   │   ├── crawler_service.py    # 爬虫服务
│   │   ├── ai_service.py         # AI分析服务
│   │   ├── data_processor.py     # 数据处理服务
│   │   ├── data_cleaner.py       # 数据清洗服务
│   │   ├── script_generator.py   # 脚本生成服务
│   │   └── login_checker.py      # 登录检查服务
│   │
│   ├── tasks/                    # Celery任务
│   │   ├── celery_app.py         # Celery应用配置
│   │   ├── crawl_tasks.py        # 爬虫任务
│   │   └── analysis_tasks.py     # 分析任务
│   │
│   ├── templates/                # HTML模板
│   │   ├── dashboard.html        # 统一控制台
│   │   ├── brands_list.html      # 品牌管理界面
│   │   ├── crawler_ui.html       # 爬虫界面
│   │   ├── mediacrawler_ui.html  # MediaCrawler界面
│   │   ├── data_analysis.html    # 数据分析界面
│   │   ├── data_viewer.html      # 数据查看界面
│   │   ├── crawl_monitor.html   # 爬取监控界面
│   │   └── reports/              # 报告模板
│   │
│   ├── logs/                     # 日志目录
│   ├── data/                     # 数据目录
│   ├── reports/                  # 报告输出目录
│   └── uploads/                  # 上传文件目录
│
├── crawlers/                     # 爬虫模块（自定义）
│   ├── base_crawler.py           # 爬虫基类
│   ├── xhs_crawler.py            # 小红书爬虫
│   ├── douyin_crawler.py         # 抖音爬虫
│   ├── weibo_crawler.py          # 微博爬虫
│   ├── zhihu_crawler.py          # 知乎爬虫
│   ├── bilibili_crawler.py       # B站爬虫
│   ├── kuaishou_crawler.py       # 快手爬虫
│   └── multi_platform_crawler.py # 多平台爬虫
│
├── MediaCrawler/                 # MediaCrawler集成
│   ├── main.py                   # MediaCrawler主程序
│   ├── media_platform/           # 各平台爬虫实现
│   ├── config/                   # 配置文件
│   ├── data/                     # 爬取数据存储
│   └── browser_data/             # 浏览器数据目录
│
├── templates/                    # 全局模板目录
├── data/                         # 数据目录
│   └── crawled_data/             # 爬取数据
│       ├── xhs/
│       ├── douyin/
│       ├── weibo/
│       ├── zhihu/
│       ├── bilibili/
│       └── kuaishou/
│
├── scripts/                      # 脚本目录
│   ├── init_database.py          # 数据库初始化
│   ├── check_environment.py      # 环境检查
│   └── create_tables.sql         # 建表SQL
│
├── docs/                         # 文档目录
│   ├── architecture.md           # 架构设计
│   ├── api_design.md             # API设计
│   ├── database_design.md        # 数据库设计
│   └── 使用指南.md                # 使用指南
│
├── config.py                     # 配置文件
├── requirements.txt              # Python依赖
├── README.md                     # 项目说明
├── QUICKSTART.md                 # 快速开始
└── 一键启动.py                    # 一键启动脚本
```

---

## 核心模块

### 1. 品牌管理模块 (`app/api/v1/brands.py`)

**功能**：
- 创建、查询、更新、删除品牌
- 品牌状态管理（活跃/非活跃/已归档）
- 品牌关键词和平台配置

**主要API**：
- `POST /api/v1/brands` - 创建品牌
- `GET /api/v1/brands` - 获取品牌列表（支持分页、筛选）
- `GET /api/v1/brands/{brand_id}` - 获取品牌详情
- `PUT /api/v1/brands/{brand_id}` - 更新品牌
- `DELETE /api/v1/brands/{brand_id}` - 删除品牌

**数据模型**：`app/models/brand.py`
- Brand: 品牌基本信息
- BrandStatus: 品牌状态枚举

### 2. 数据采集模块

#### 2.1 爬虫任务管理 (`app/api/v1/crawl_tasks.py`)

**功能**：
- 创建爬虫任务
- 查询任务状态和进度
- 任务管理（启动、停止、重试）

**主要API**：
- `POST /api/v1/brands/{brand_id}/crawl` - 启动爬虫任务
- `GET /api/v1/crawl-tasks` - 获取任务列表
- `GET /api/v1/crawl-tasks/{task_id}` - 获取任务详情

**数据模型**：`app/models/crawl_task.py`
- CrawlTask: 爬虫任务信息
- TaskStatus: 任务状态枚举

#### 2.2 MediaCrawler集成 (`app/api/v1/mediacrawler_ui.py`)

**功能**：
- 集成MediaCrawler进行多平台爬取
- 实时监控爬取进度
- 输出流式显示

**主要API**：
- `POST /api/v1/mediacrawler/start` - 启动MediaCrawler爬取
- `GET /api/v1/mediacrawler/crawl/monitor/{process_id}` - 监控页面
- `GET /api/v1/mediacrawler/crawl/output/{process_id}` - 获取输出
- `POST /api/v1/mediacrawler/crawl/stop/{process_id}` - 停止爬取

**服务层**：`app/services/crawler_service.py`
- CrawlerService: 爬虫服务封装
- ScriptGenerator: 脚本生成器

### 3. AI分析模块 (`app/api/v1/analysis_tasks.py`)

**功能**：
- 创建分析任务
- 调用AI服务进行数据分析
- 情感分析、主题提取、关键词分析、深度洞察

**主要API**：
- `POST /api/v1/brands/{brand_id}/analyze` - 启动分析任务
- `GET /api/v1/analysis-tasks` - 获取分析任务列表
- `GET /api/v1/analysis-tasks/{task_id}` - 获取分析结果

**服务层**：`app/services/ai_service.py`
- AIService: 统一封装AI调用逻辑
- **接口配置**：优先使用LLM聚合网关（通过自定义Base URL接入），实现多模型负载均衡与故障转移
- **兼容性**：完全兼容OpenAI SDK格式，支持动态切换模型
- **备用方案**：支持直接调用OpenAI、Anthropic、Gemini、本地LLM

**数据模型**：`app/models/analysis_task.py`
- AnalysisTask: 分析任务信息

### 4. 报告生成模块 (`app/api/v1/reports.py`)

**功能**：
- 生成品牌分析报告
- 支持PDF、Word、Excel格式
- 报告模板管理

**主要API**：
- `POST /api/v1/brands/{brand_id}/reports` - 生成报告
- `GET /api/v1/reports` - 获取报告列表
- `GET /api/v1/reports/{report_id}` - 下载报告

**数据模型**：`app/models/report.py`
- Report: 报告信息

### 5. 数据查看模块 (`app/api/v1/data_viewer.py`)

**功能**：
- 查看爬取的数据
- 数据筛选和搜索
- JSON数据查看器

**主要API**：
- `GET /api/v1/data` - 获取数据列表
- `GET /api/v1/brands/{brand_id}/data` - 获取品牌数据
- `GET /api/v1/brands/{brand_id}/data/view` - 数据查看页面

### 6. 统一控制台 (`app/api/v1/dashboard.py`)

**功能**：
- 系统概览
- 快速访问各功能模块
- 任务监控

**主要API**：
- `GET /api/v1/dashboard` - 控制台页面

---

## API架构

### API版本管理

- 当前版本：`v1`
- 基础路径：`/api/v1`

### API路由结构

```
/api/v1/
├── brands/                       # 品牌管理
│   ├── GET /                     # 获取品牌列表
│   ├── POST /                    # 创建品牌
│   ├── GET /{brand_id}           # 获取品牌详情
│   ├── PUT /{brand_id}           # 更新品牌
│   └── DELETE /{brand_id}        # 删除品牌
│
├── crawl-tasks/                  # 爬虫任务
│   ├── GET /                     # 获取任务列表
│   └── GET /{task_id}            # 获取任务详情
│
├── brands/{brand_id}/
│   ├── POST /crawl               # 启动爬虫任务
│   ├── POST /analyze             # 启动分析任务
│   └── GET /data                 # 获取品牌数据
│
├── analysis-tasks/               # 分析任务
│   ├── GET /                     # 获取任务列表
│   └── GET /{task_id}            # 获取分析结果
│
├── reports/                       # 报告
│   ├── GET /                     # 获取报告列表
│   └── GET /{report_id}          # 下载报告
│
├── mediacrawler/                 # MediaCrawler集成
│   ├── POST /start               # 启动爬取
│   ├── GET /crawl/monitor/{process_id}  # 监控页面
│   ├── GET /crawl/output/{process_id}  # 获取输出
│   └── POST /crawl/stop/{process_id}   # 停止爬取
│
└── dashboard/                    # 统一控制台
    └── GET /                     # 控制台页面
```

### 响应格式

**成功响应**：
```json
{
  "code": 200,
  "message": "success",
  "data": { ... }
}
```

**错误响应**：
```json
{
  "code": 400/404/500,
  "message": "错误信息",
  "error": "详细错误描述"
}
```

---

## 数据库设计

### MySQL数据库（结构化数据）

#### 1. brands 表（品牌表）

| 字段 | 类型 | 说明 |
|------|------|------|
| id | INT | 主键 |
| name | VARCHAR(100) | 品牌名称 |
| description | TEXT | 品牌描述 |
| keywords | JSON | 关键词列表 |
| platforms | JSON | 支持的平台列表 |
| status | ENUM | 状态（active/inactive/archived） |
| created_at | DATETIME | 创建时间 |
| updated_at | DATETIME | 更新时间 |

#### 2. crawl_tasks 表（爬虫任务表）

| 字段 | 类型 | 说明 |
|------|------|------|
| id | INT | 主键 |
| brand_id | INT | 品牌ID（外键，CASCADE） |
| platform | VARCHAR(50) | 平台名称 |
| status | ENUM | 任务状态 |
| keyword | VARCHAR(200) | 搜索关键词 |
| max_items | INT | 最大采集数量 |
| total_items | INT | 总数据量 |
| crawled_items | INT | 已采集数量 |
| progress | INT | 进度百分比 |
| created_at | DATETIME | 创建时间 |
| started_at | DATETIME | 开始时间 |
| completed_at | DATETIME | 完成时间 |

#### 3. analysis_tasks 表（分析任务表）

| 字段 | 类型 | 说明 |
|------|------|------|
| id | INT | 主键 |
| brand_id | INT | 品牌ID（外键，CASCADE） |
| status | ENUM | 任务状态 |
| analysis_type | VARCHAR(50) | 分析类型 |
| include_sentiment | BOOLEAN | 包含情感分析 |
| include_topics | BOOLEAN | 包含主题提取 |
| include_keywords | BOOLEAN | 包含关键词分析 |
| include_insights | BOOLEAN | 包含深度洞察 |
| celery_task_id | VARCHAR(255) | Celery任务ID |
| progress | INT | 进度百分比 |
| created_at | DATETIME | 创建时间 |
| started_at | DATETIME | 开始时间 |
| completed_at | DATETIME | 完成时间 |

#### 4. reports 表（报告表）

| 字段 | 类型 | 说明 |
|------|------|------|
| id | INT | 主键 |
| brand_id | INT | 品牌ID（外键，CASCADE） |
| analysis_task_id | INT | 分析任务ID（外键，SET NULL） |
| report_type | VARCHAR(50) | 报告类型 |
| format | VARCHAR(20) | 文件格式 |
| file_path | VARCHAR(500) | 文件路径 |
| file_size | BIGINT | 文件大小 |
| status | ENUM | 状态 |
| created_at | DATETIME | 创建时间 |
| completed_at | DATETIME | 完成时间 |

### MongoDB数据库（非结构化数据）

#### raw_data 集合（原始爬取数据）

```javascript
{
  _id: ObjectId,
  brand_id: Number,           // 品牌ID
  platform: String,            // 平台: xhs, douyin, weibo, zhihu
  task_id: Number,             // 爬虫任务ID
  content_type: String,        // 内容类型: post, comment, video
  content_id: String,          // 平台上的内容ID
  title: String,               // 标题
  content: String,             // 正文内容
  author: Object,              // 作者信息
  publish_time: Date,          // 发布时间
  engagement: Object,          // 互动数据（点赞、评论、分享等）
  media: Object,               // 媒体信息（图片、视频）
  raw_data: Object,            // 原始数据（完整JSON）
  crawled_at: Date             // 爬取时间
}
```

#### analysis_results 集合（分析结果）

```javascript
{
  _id: ObjectId,
  brand_id: Number,            // 品牌ID
  analysis_task_id: Number,    // 分析任务ID
  analysis_type: String,        // 分析类型
  sentiment: Object,            // 情感分析结果
  topics: Array,                // 主题列表
  keywords: Array,              // 关键词列表
  insights: String,            // 深度洞察
  created_at: Date              // 创建时间
}
```

---

## 数据流

### 1. 数据采集流程

```
用户创建品牌
    ↓
配置关键词和平台
    ↓
启动爬虫任务
    ↓
MediaCrawler执行爬取
    ↓
数据清洗和去重
    ↓
存储到MongoDB
    ↓
更新任务状态
```

### 2. AI分析流程

```
选择品牌
    ↓
启动分析任务
    ↓
从MongoDB读取数据
    ↓
数据预处理
    ↓
调用AI服务分析
    ├── 情感分析
    ├── 主题提取
    ├── 关键词分析
    └── 深度洞察
    ↓
存储分析结果到MongoDB
    ↓
更新任务状态
```

### 3. 报告生成流程

```
选择品牌和分析任务
    ↓
从MongoDB读取分析结果
    ↓
数据可视化处理
    ↓
填充报告模板
    ↓
生成PDF/Word/Excel
    ↓
保存文件
    ↓
更新报告状态
```

---

## 部署架构

### 开发环境

```
┌─────────────────┐
│  FastAPI服务    │  (localhost:8000)
│  - 开发模式      │
│  - 热重载        │
└─────────────────┘
         │
    ┌────┴────┐
    │         │
┌───▼───┐ ┌──▼───┐
│ MySQL │ │MongoDB│
└───────┘ └──────┘
```

### 生产环境（推荐）

```
┌─────────────────┐
│  Nginx反向代理  │
└────────┬─────────┘
         │
┌────────▼─────────┐
│  FastAPI服务     │  (Gunicorn/Uvicorn)
│  - 多进程        │
│  - 负载均衡      │
└────────┬─────────┘
         │
    ┌────┴────┐
    │         │
┌───▼───┐ ┌──▼────┐ ┌──────┐
│ MySQL │ │MongoDB│ │Redis │
└───────┘ └───────┘ └──────┘
         │
┌────────▼─────────┐
│  Celery Worker   │
│  - 异步任务      │
└──────────────────┘
```

### 启动方式

**方式1：一键启动（推荐）**
```bash
python 一键启动.py
```

**方式2：批处理文件**
```bash
启动FastAPI.bat
启动Celery.bat
```

**方式3：手动启动**
```bash
# 终端1：启动FastAPI
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# 终端2：启动Celery
celery -A app.tasks.celery_app worker --loglevel=info
```

---

## 关键配置文件

### config.py

主要配置项：
- **LLM聚合网关配置**（优先使用）：
  - `LLM_API_BASE`: AI服务接口地址（例如: `https://xy.xiaoxu030.xyz:8888/v1`）
  - `LLM_API_KEY`: 聚合服务的分发令牌（通常以 `sk-` 开头）
  - `LLM_MODEL_NAME`: 默认使用的模型名称（如 `gpt-4o-mini`）
- 数据库连接（MySQL、MongoDB、Redis）
- AI服务配置（OpenAI、Anthropic、Gemini，作为备用方案）
- 文件存储路径
- 日志配置

### .gitignore

忽略文件：
- `__pycache__/` - Python缓存
- `*.pyc` - 编译文件
- `*.log` - 日志文件
- `venv/` - 虚拟环境
- `MediaCrawler/python_env/` - MediaCrawler虚拟环境
- `data/crawled_data/` - 爬取数据
- `uploads/` - 上传文件
- `reports/` - 报告文件

---

## 依赖管理

### requirements.txt

主要依赖：
- fastapi - Web框架
- uvicorn - ASGI服务器
- sqlalchemy - ORM
- pymysql - MySQL驱动
- pymongo - MongoDB驱动
- redis - Redis客户端
- celery - 异步任务
- pandas - 数据处理
- openai - OpenAI API
- anthropic - Anthropic API
- google-generativeai - Gemini API
- loguru - 日志管理

---

## 安全考虑

1. **API认证**：支持JWT Token认证（可选）
2. **数据验证**：使用Pydantic进行数据验证
3. **SQL注入防护**：使用ORM防止SQL注入
4. **CORS配置**：配置跨域访问策略
5. **敏感信息**：使用环境变量存储密钥

---

## 性能优化

1. **数据库连接池**：使用SQLAlchemy连接池
2. **Redis缓存**：缓存热点数据
3. **异步任务**：使用Celery处理耗时任务
4. **数据分页**：API支持分页查询
5. **日志轮转**：日志文件自动轮转和清理

---

## 扩展性

1. **模块化设计**：各模块独立，易于扩展
2. **插件化爬虫**：支持添加新平台爬虫
3. **多AI服务**：支持切换不同的AI服务
4. **分布式部署**：支持多节点部署
5. **微服务化**：可拆分为独立微服务

---

## 维护和监控

1. **日志系统**：使用Loguru记录详细日志
2. **健康检查**：`/health` 端点
3. **错误处理**：全局异常处理器
4. **任务监控**：Celery Flower（可选）

---

## 文档资源

- `README.md` - 项目说明
- `QUICKSTART.md` - 快速开始指南
- `docs/architecture.md` - 详细架构设计
- `docs/api_design.md` - API接口文档
- `docs/database_design.md` - 数据库设计文档
- `docs/使用指南.md` - 使用指南

---

## 版本信息

- **项目名称**：品牌分析系统
- **版本**：v1.0.0
- **最后更新**：2026-01-06

---

## 联系方式

如有问题或建议，请查看项目文档或提交Issue。


