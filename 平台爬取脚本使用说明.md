# 平台爬取脚本使用说明

## 概述

系统现在为每个平台的每次爬取任务自动生成一个独立的Python脚本文件，然后在Web页面爬取时使用这些脚本文件执行爬取。

## 工作流程

1. **用户提交爬取请求**
   - 在Web页面选择平台、输入关键词、设置参数
   - 点击"开始爬取"

2. **系统生成脚本**
   - 根据平台和参数自动生成独立的Python脚本
   - 脚本保存在 `crawl_scripts/` 目录下
   - 文件名格式：`crawl_{平台}_{时间戳}.py`

3. **执行脚本**
   - 系统使用Python执行生成的脚本
   - 脚本内部调用MediaCrawler进行实际爬取
   - 实时捕获输出并显示在监控页面

## 脚本目录结构

```
项目根目录/
├── crawl_scripts/          # 自动生成的爬取脚本目录
│   ├── crawl_xhs_20260104_120000.py
│   ├── crawl_douyin_20260104_120100.py
│   ├── crawl_weibo_20260104_120200.py
│   └── ...
└── ...
```

## 脚本文件内容

每个脚本文件包含：
- 平台信息
- 关键词
- 最大爬取数量
- 完整的MediaCrawler命令参数
- 错误处理逻辑

## 优势

1. **独立性**：每个爬取任务都有独立的脚本文件
2. **可追溯性**：可以查看历史爬取任务的脚本
3. **可调试性**：可以直接运行脚本文件进行测试
4. **可复用性**：可以手动修改脚本后重新运行

## 脚本示例

```python
"""
自动生成的MediaCrawler爬取脚本
平台: xhs (xhs)
关键词: 华为,小米
最大数量: 10
生成时间: 2026-01-04 12:00:00
"""
import subprocess
import sys
from pathlib import Path

# MediaCrawler路径
mediacrawler_path = Path(r"C:\Users\Yu\cursorProjects\githup\MediaCrawler")

# 命令参数
cmd = [
    "python",
    "C:\\Users\\Yu\\cursorProjects\\githup\\MediaCrawler\\main.py",
    "--platform", "xhs",
    "--lt", "qrcode",
    "--type", "search",
    "--keywords", "华为,小米",
    "--save_data_option", "json",
    "--note_type", "all",
    "--get_comment", "yes",
    "--get_sub_comment", "no"
]

# 工作目录
work_dir = str(mediacrawler_path)

# 执行命令
print("=" * 70)
print(f"开始爬取: xhs")
print(f"关键词: 华为,小米")
print(f"最大数量: 10")
print("=" * 70)
print(f"执行命令: {' '.join(cmd)}")
print("=" * 70)

try:
    result = subprocess.run(
        cmd,
        cwd=work_dir,
        capture_output=False,
        text=True,
        encoding='utf-8',
        errors='ignore'
    )
    
    print("=" * 70)
    print(f"爬取完成，返回码: {result.returncode}")
    print("=" * 70)
    
    if result.returncode != 0:
        print(f"错误: 爬取失败，返回码 {result.returncode}")
        sys.exit(result.returncode)
    else:
        print("爬取成功！")
        sys.exit(0)
        
except Exception as e:
    print(f"执行失败: {e}")
    sys.exit(1)
```

## 手动运行脚本

如果需要手动运行脚本进行测试：

```bash
# Windows
python crawl_scripts\crawl_xhs_20260104_120000.py

# Linux/Mac
python crawl_scripts/crawl_xhs_20260104_120000.py
```

## 注意事项

1. **脚本清理**：脚本文件会一直保留，如需清理可以手动删除 `crawl_scripts/` 目录下的旧文件
2. **脚本路径**：脚本路径会保存在进程数据中，可以通过监控页面查看
3. **脚本权限**：确保脚本文件有执行权限（Windows通常不需要特殊权限）

## 故障排查

如果爬取失败，可以：
1. 查看生成的脚本文件内容
2. 手动运行脚本测试
3. 检查脚本中的路径和参数是否正确
4. 查看进程数据中的 `script_path` 字段



